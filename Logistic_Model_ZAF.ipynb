{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Libraries and Enviroment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary libraries and Account IDs\n",
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "\n",
    "database_name = \"insta_db\"\n",
    "\n",
    "from pyathena import connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set S3 staging directory -- this is a temporary directory used for Athena queries\n",
    "s3_staging_dir = \"s3://{0}/athena/staging\".format(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Connections\n",
    "conn = connect(region_name=region, s3_staging_dir=s3_staging_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize Data for Logistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement\n",
    "statement = \"\"\"\n",
    "DROP TABLE IF EXISTS insta_db.final_parquet\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(statement, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement\n",
    "statement = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS insta_db.final_parquet\n",
    "WITH (format = 'PARQUET', \\\n",
    "        external_location = 's3://{}/parquet/final_parquet') AS \n",
    "with orders as(\n",
    "select *\n",
    ", sum(coalesce(days_since_prior_order,0)) over (partition by user_id order by order_number asc) as days\n",
    "FROM insta_db.orders_parquet\n",
    "), products as (\n",
    "select t.*, p.product_name, d.department, a.aisle\n",
    "from (\n",
    "SELECT *\n",
    "FROM insta_db.order_products_prior_parquet\n",
    ") t\n",
    "left join insta_db.products_parquet p\n",
    "on t.product_id = p.product_id\n",
    "left join insta_db.departments_parquet d\n",
    "on p.department_id = d.department_id\n",
    "left join insta_db.aisles_parquet a\n",
    "on p.aisle_id = a.aisle_id\n",
    ")\n",
    "select p.reordered\n",
    ", p.product_name\n",
    ", p.department\n",
    ", p.aisle\n",
    ", coalesce( days - lag(days) over (partition by o.user_id, p.product_id order by o.order_number) , 0) as days_since_previous_order\n",
    "from products p\n",
    "join orders o\n",
    "on p.order_id = o.order_id\n",
    "order by p.product_id,\n",
    "o.order_number ASC\n",
    "\"\"\".format(\n",
    "    bucket\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.read_sql(statement, conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL statement\n",
    "statement = \"\"\"\n",
    "select *\n",
    ", pow(days_since_previous_order,2) as days_2\n",
    ", pow(days_since_previous_order,3) as days_3\n",
    ", pow(days_since_previous_order,4) as days_4\n",
    ", pow(days_since_previous_order,5) as days_5\n",
    ", pow(days_since_previous_order,6) as days_6\n",
    ", pow(days_since_previous_order,7) as days_7\n",
    ", pow(days_since_previous_order,8) as days_8\n",
    "from insta_db.final_parquet --limit 200000\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_sql(statement, conn)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's Create Dummies\n",
    "\n",
    "# product = pd.get_dummies(df['product_name'],drop_first=True)\n",
    "department = pd.get_dummies(df['department'],drop_first=True)\n",
    "aisle = pd.get_dummies(df['aisle'],drop_first=True)\n",
    "\n",
    "# Concatenate new columns with dummies to df\n",
    "df = pd.concat([df, department, aisle], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Logistic Regression / Propensisty Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Check for missing values using seaborn.\n",
    "sns.heatmap(df.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='reordered', data = df , palette = 'Set1').set(title= 'Number of Repurchased Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.DataFrame(df['reordered'])\n",
    "Y = Y.apply(pd.to_numeric)\n",
    "\n",
    "X = df.drop(['reordered', 'product_name', 'department', 'aisle', 'days_6','days_7','days_8'], axis = 1)\n",
    "X = X.apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Train Split Data \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Model\n",
    "logmodel = LogisticRegression(class_weight = 'balanced', max_iter =10000000)\n",
    "logmodel.fit(X_train, y_train.values.ravel())    \n",
    "predictions_log= logmodel.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation\n",
    "print(classification_report(y_test,predictions_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our model has a 97% precision rate for not repurchased and a 100% precision rate for repurchase. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "importance = logmodel.coef_.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot of features\n",
    "\n",
    "pyplot.rcParams[\"figure.figsize\"]=(10,10)\n",
    "pyplot.barh(X.columns,importance, color = 'g')\n",
    "pyplot.title(\"Feature Importance\")\n",
    "pyplot.xlabel(\"Score\")\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What will we do with this model\n",
    "\n",
    "With this propensisty model.  We will use it to score customer product purchase history to determine which of thier past purchases have the highest propensity to be repurchased. This sorted list of possible repurchase products will be used for remarketing purposes for the customer across the website, in emails, and on direct mail flyers. The hopes is to bring the customer back and in so doing, we get the opportunity to cross sell the customer on our other products. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
